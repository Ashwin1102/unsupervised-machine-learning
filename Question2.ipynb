{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20ng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('20ng.csv')\n",
    "df['index'] = df['index'].str.replace(r'\\d+$', '', regex=True)\n",
    "df.dropna(inplace=True)\n",
    "df = df[:1000]\n",
    "\n",
    "vectorizer = TfidfVectorizer(use_idf=True)\n",
    "sparse_matrix = vectorizer.fit_transform(df['content'])\n",
    "\n",
    "sparse_df = pd.DataFrame.sparse.from_spmatrix(sparse_matrix, columns=vectorizer.get_feature_names_out())\n",
    "sparse_df['index'] = df['index'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = sparse_df.columns[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_word_distribution(sentences, words):\n",
    "    text = \" \".join(sentences)\n",
    "    word_list = text.split()\n",
    "    counter = {}\n",
    "    words_set = set(words)\n",
    "\n",
    "    for i in words_set:\n",
    "        if i not in counter:\n",
    "            counter[i] = 0\n",
    "\n",
    "    for word in word_list:\n",
    "        if word not in counter:\n",
    "            continue\n",
    "        counter[word] += 1\n",
    "    \n",
    "    return counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_x = sparse_df.drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_array = sparse_x.iloc[0].to_numpy(dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Documents:   1%|          | 6/1000 [00:03<07:14,  2.29it/s]C:\\Users\\ashwi\\AppData\\Local\\Temp\\ipykernel_54104\\973745537.py:42: RuntimeWarning: invalid value encountered in divide\n",
      "  ps_vals = ps_vals / ps_vals.sum()\n",
      "Processing Documents: 100%|██████████| 1000/1000 [06:20<00:00,  2.63it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "summaries = []\n",
    "\n",
    "for df_i, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Processing Documents\"):\n",
    "    text = row['content']\n",
    "    sentences = sent_tokenize(text)\n",
    "    pd_vals = sparse_x.iloc[df_i].to_numpy(dtype=float)\n",
    "    kl_vals = []\n",
    "    epsilon = 1e-10\n",
    "    sub_sentences = []\n",
    "    \n",
    "\n",
    "    for sentence_ind in tqdm(range(len(sentences)), desc=f\"Processing Sentences for doc {df_i+1}\", leave=False):\n",
    "        sub_sentences.append(sentences[sentence_ind])\n",
    "        counter = get_word_distribution(sub_sentences, words)\n",
    "        ps_vals = np.array(list(counter.values()))\n",
    "        ps_vals = ps_vals / ps_vals.sum()\n",
    "        pd_vals_safe = np.clip(pd_vals, epsilon, None)\n",
    "        ps_vals_safe = np.clip(ps_vals, epsilon, None)\n",
    "        final_arr = pd_vals_safe * np.log(pd_vals_safe / ps_vals_safe)\n",
    "        final_arr = np.nan_to_num(final_arr)\n",
    "        kl_vals.append(np.sum(final_arr))\n",
    "\n",
    "    kl_vals = np.array(kl_vals)\n",
    "    summary = \" \".join(sentences[0:np.argmin(kl_vals) + 1])\n",
    "    summaries.append(summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "vectorizer = TfidfVectorizer(use_idf=False)\n",
    "\n",
    "sparse_matrix = vectorizer.fit_transform(df['content'])\n",
    "lda = LatentDirichletAllocation(n_components=30, random_state=42)\n",
    "topics = lda.fit_transform(sparse_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = lda.fit_transform(sparse_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Documents: 100%|██████████| 1000/1000 [00:24<00:00, 40.85it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "summaries = []\n",
    "count = 0\n",
    "for df_i, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Processing Documents\"):\n",
    "    text = df.loc[df_i, 'content']\n",
    "    sentences = sent_tokenize(text)\n",
    "    doc_topic_distribution = topics[count]\n",
    "    curr_sentence = []\n",
    "    kls = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        curr_sentence.append(sentence)\n",
    "        curr_sentence_joined = \" \".join(curr_sentence)\n",
    "        sparse_matrix = vectorizer.transform([curr_sentence_joined])\n",
    "        sub_sen_topic_distribution = lda.transform(sparse_matrix)\n",
    "        kl_score = np.sum(doc_topic_distribution * np.log(doc_topic_distribution/ sub_sen_topic_distribution))\n",
    "        kls.append(kl_score)\n",
    "    kls = np.array(kls)\n",
    "    summaries.append(\" \".join(sentences[0:np.argmin(kls) + 1]))\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DUC2001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('DUC2001.csv')\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "vectorizer = TfidfVectorizer(use_idf=True)\n",
    "sparse_matrix = vectorizer.fit_transform(df['content'])\n",
    "\n",
    "sparse_df = pd.DataFrame.sparse.from_spmatrix(sparse_matrix, columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 5819)"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Documents:  17%|█▋        | 51/300 [00:05<00:26,  9.51it/s]C:\\Users\\ashwi\\AppData\\Local\\Temp\\ipykernel_54104\\2453780924.py:17: RuntimeWarning: invalid value encountered in divide\n",
      "  ps_vals = ps_vals / ps_vals.sum()\n",
      "Processing Documents: 100%|██████████| 300/300 [00:30<00:00,  9.90it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "summaries = []\n",
    "\n",
    "for df_i, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Processing Documents\"):\n",
    "    text = row['content']\n",
    "    sentences = sent_tokenize(text)\n",
    "    pd_vals = sparse_x.iloc[df_i].to_numpy(dtype=float)\n",
    "    kl_vals = []\n",
    "    epsilon = 1e-10\n",
    "    sub_sentences = []\n",
    "    \n",
    "\n",
    "    for sentence_ind in tqdm(range(len(sentences)), desc=f\"Processing Sentences for doc {df_i+1}\", leave=False):\n",
    "        sub_sentences.append(sentences[sentence_ind])\n",
    "        counter = get_word_distribution(sub_sentences, words)\n",
    "        ps_vals = np.array(list(counter.values()))\n",
    "        ps_vals = ps_vals / ps_vals.sum()\n",
    "        pd_vals_safe = np.clip(pd_vals, epsilon, None)\n",
    "        ps_vals_safe = np.clip(ps_vals, epsilon, None)\n",
    "        final_arr = pd_vals_safe * np.log(pd_vals_safe / ps_vals_safe)\n",
    "        final_arr = np.nan_to_num(final_arr)\n",
    "        kl_vals.append(np.sum(final_arr))\n",
    "\n",
    "    kl_vals = np.array(kl_vals)\n",
    "    summary = \" \".join(sentences[0:np.argmin(kl_vals) + 1])\n",
    "    summaries.append(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1: 0.2233361130154441\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1'])\n",
    "rouge_1 = []\n",
    "expected_summaries = df['summary'].tolist()\n",
    "\n",
    "for expected, generated in zip(expected_summaries, summaries):\n",
    "    scores = scorer.score(expected, generated)\n",
    "    rouge_1.append(scores['rouge1'].fmeasure)\n",
    "\n",
    "avg_rouge_1 = sum(rouge_1) / len(rouge_1)\n",
    "    \n",
    "print(f\"ROUGE-1:\",  avg_rouge_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Some 40 members of Congress have joined with ...</td>\n",
       "      <td>A coalition of members of Congress announced W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Multitudes of native peoples, tourists and sc...</td>\n",
       "      <td>Thousands of peole prayed, cheered, danced, be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Population experts say that little would chan...</td>\n",
       "      <td>If the two sides trying to force changes in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The unofficial tornado season runs from April...</td>\n",
       "      <td>Rumbling spring thunderstorms have announced t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>William Gray, a hurricane expert, predicts mo...</td>\n",
       "      <td>A hurricane expert predicts a turbulent summer...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0   Some 40 members of Congress have joined with ...   \n",
       "1   Multitudes of native peoples, tourists and sc...   \n",
       "2   Population experts say that little would chan...   \n",
       "3   The unofficial tornado season runs from April...   \n",
       "4   William Gray, a hurricane expert, predicts mo...   \n",
       "\n",
       "                                             summary  \n",
       "0  A coalition of members of Congress announced W...  \n",
       "1  Thousands of peole prayed, cheered, danced, be...  \n",
       "2  If the two sides trying to force changes in th...  \n",
       "3  Rumbling spring thunderstorms have announced t...  \n",
       "4  A hurricane expert predicts a turbulent summer...  "
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "vectorizer = TfidfVectorizer(use_idf=False)\n",
    "\n",
    "sparse_matrix = vectorizer.fit_transform(df['content'])\n",
    "lda = LatentDirichletAllocation(n_components=30, random_state=42)\n",
    "topics = lda.fit_transform(sparse_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = lda.fit_transform(sparse_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Documents: 100%|██████████| 300/300 [00:01<00:00, 248.39it/s]\n"
     ]
    }
   ],
   "source": [
    "summaries = []\n",
    "count = 0\n",
    "for df_i, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Processing Documents\"):\n",
    "    text = df.loc[df_i, 'content']\n",
    "    sentences = sent_tokenize(text)\n",
    "    doc_topic_distribution = topics[count]\n",
    "    curr_sentence = []\n",
    "    kls = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        curr_sentence.append(sentence)\n",
    "        curr_sentence_joined = \" \".join(curr_sentence)\n",
    "        sparse_matrix = vectorizer.transform([curr_sentence_joined])\n",
    "        sub_sen_topic_distribution = lda.transform(sparse_matrix)\n",
    "        kl_score = np.sum(doc_topic_distribution * np.log(doc_topic_distribution/ sub_sen_topic_distribution))\n",
    "        kls.append(kl_score)\n",
    "    kls = np.array(kls)\n",
    "    summaries.append(\" \".join(sentences[0:np.argmin(kls) + 1]))\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1: 0.2521454000827042\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1'])\n",
    "rouge_1 = []\n",
    "    \n",
    "for expected, generated in zip(expected_summaries, summaries):\n",
    "    scores = scorer.score(expected, generated)\n",
    "    rouge_1.append(scores['rouge1'].fmeasure)\n",
    "\n",
    "avg_rouge_1 = sum(rouge_1) / len(rouge_1)\n",
    "    \n",
    "print(f\"ROUGE-1:\",  avg_rouge_1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
